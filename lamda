import boto3
import os
import datetime
import time
import threading

# Constants
LOG_GROUPS = ["/aws/lambda/getshoppinglist", "/aws/lambda/CORS_function"]  # List of log group names
DESTINATION_BUCKET = "logsexportbucketdemo"  # Specify your S3 bucket here
PREFIX = "stav"  # Prefix for organizing logs in the bucket
NDAYS = 170  # Set to 170 days for the archive threshold
MAX_RETRIES = 3  # Max retry attempts for each export task

# Get the current time and calculate the date range for logs older than 170 days
currentTime = datetime.datetime.now()
StartDate = currentTime - datetime.timedelta(days=NDAYS)
EndDate = StartDate + datetime.timedelta(days=1)  # Archive one-day span of logs

# Convert dates to timestamps in milliseconds
fromDate = int(StartDate.timestamp() * 1000)
toDate = int(EndDate.timestamp() * 1000)

def create_export_task(client, group_name, from_date, to_date, destination_bucket, bucket_prefix, retries=0):
    try:
        # Create the export task for the current log group
        response = client.create_export_task(
            logGroupName=group_name,
            fromTime=from_date,
            to=to_date,
            destination=destination_bucket,
            destinationPrefix=bucket_prefix
        )
        print(f"Export task created for {group_name}: {response}")
    except client.exceptions.LimitExceededException as e:
        if retries < MAX_RETRIES:
            print(f"LimitExceededException: Resource limit exceeded for {group_name}. Attempt {retries + 1}/{MAX_RETRIES}. Retrying in 30 seconds...")
            time.sleep(30)  # Wait before retrying
            create_export_task(client, group_name, from_date, to_date, destination_bucket, bucket_prefix, retries + 1)
        else:
            print(f"Failed to create export task for {group_name} after {MAX_RETRIES} attempts.")
    except Exception as e:
        print(f"Error creating export task for {group_name}: {e}")

def lambda_handler(event, context):
    client = boto3.client('logs')

    threads = []

    # Loop through each log group and create export tasks in parallel
    for group_name in LOG_GROUPS:
        # Clean up the log group name for the S3 prefix (replace '/' with '_')
        bucket_prefix = os.path.join(PREFIX, group_name.replace('/', '_'), StartDate.strftime('%Y/%m/%d'))

        # Debug logging
        print(f"Archiving logs older than {NDAYS} days")
        print(f"Log Group: {group_name}")
        print(f"Time Range: {StartDate} to {EndDate}")
        print(f"Destination Bucket: {DESTINATION_BUCKET}")
        print(f"S3 Prefix: {bucket_prefix}")

        # Create a new thread for each log group export task
        thread = threading.Thread(target=create_export_task, args=(client, group_name, fromDate, toDate, DESTINATION_BUCKET, bucket_prefix))
        thread.start()
        threads.append(thread)

    # Wait for all threads to complete
    for thread in threads:
        thread.join()

